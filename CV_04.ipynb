{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d01a1655",
   "metadata": {
    "id": "d01a1655"
   },
   "source": [
    "# 1. Exercise of saving model parameter etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047c65cd",
   "metadata": {
    "id": "047c65cd"
   },
   "outputs": [],
   "source": [
    "# import library\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeb9095",
   "metadata": {
    "id": "2eeb9095"
   },
   "outputs": [],
   "source": [
    "# MNIST data download\n",
    "# the below codes don't work\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_set(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# alternative\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train.astype('float32'), x_test.astype('float32')\n",
    "x_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1, 784])\n",
    "x_train, x_test = x_train / 255., x_test / 255.\n",
    "y_train, y_test = tf.one_hot(y_train, depth=10), tf.one_hot(y_test, depth=10)\n",
    "\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat().shuffle(60000).batch(50)\n",
    "train_data_iter = iter(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2070e1b",
   "metadata": {
    "id": "e2070e1b"
   },
   "source": [
    "## Define CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52746f76",
   "metadata": {
    "id": "52746f76"
   },
   "outputs": [],
   "source": [
    "# define CNN model\n",
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # the first Convolution layer\n",
    "        # apply 32 filters with 5x5 Kernel size\n",
    "        self.conv_layer_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=5, strides=1, padding='same', activation='relu')\n",
    "        self.pool_layer_1 = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=2)\n",
    "        \n",
    "        # the second Convolution layer\n",
    "        # apply 64 filters with 5x5 Kernel size\n",
    "        self.conv_layer_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=5, strides=1, padding='same', activation='relu')\n",
    "        self.pool_layer_2 = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=2)\n",
    "        \n",
    "        #Fully connected Layer\n",
    "        # Change 7x7 64 activation maps to 1024 features\n",
    "        self.flatten_layer = tf.keras.layers.Flatten()\n",
    "        self.fc_layer_1 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        \n",
    "        # Output Layer\n",
    "        # Change 1024 features as 10 classes by one-hot encodding\n",
    "        self.output_layer = tf.keras.layers.Dense(10, activation=None)\n",
    "        \n",
    "    def call(self,x):\n",
    "        # reshape MNIST data as 3 dims\n",
    "        x_image = tf.reshape(x, [-1,28,28,1])\n",
    "        # 28x28x1 -> 28x28x32\n",
    "        h_conv1 = self.conv_layer_1(x_image)\n",
    "        # 28x28x32 -> 14x14x32\n",
    "        h_pool1 = self.pool_layer_1(h_conv1)\n",
    "        # 14x14x32 -> 14x14x64\n",
    "        h_conv2 = self.conv_layer_2(h_pool1)\n",
    "        # 14x14x64 -> 7x7x64\n",
    "        h_pool2 = self.pool_layer_2(h_conv2)\n",
    "        # 7x7x64(3136) -> 1024\n",
    "        h_pool2_flat = self.flatten_layer(h_pool2)\n",
    "        h_fc1 = self.fc_layer_1(h_pool2_flat)\n",
    "        # 1024 -> 10\n",
    "        logits = self.output_layer(h_fc1)\n",
    "        y_pred = tf.nn.softmax(logits)\n",
    "\n",
    "        return y_pred, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc82fd9",
   "metadata": {
    "id": "edc82fd9"
   },
   "outputs": [],
   "source": [
    "# difine cross-entropy loss function\n",
    "def cross_entropy_loss(logits, y):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f6218",
   "metadata": {
    "id": "305f6218"
   },
   "outputs": [],
   "source": [
    "# define optimizer for optimization\n",
    "optimizer = tf.optimizers.Adam(1e-4)\n",
    "\n",
    "# function for optimization\n",
    "def train_step(model, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred, logits = model(x)\n",
    "        loss = cross_entropy_loss(logits, y)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "# function for printing model's accuracy\n",
    "def compute_accuracy(y_pred, y):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f57341",
   "metadata": {
    "id": "63f57341",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# declare model\n",
    "CNN_model = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b378d5f6",
   "metadata": {
    "id": "b378d5f6"
   },
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55e93c",
   "metadata": {
    "id": "7a55e93c"
   },
   "outputs": [],
   "source": [
    "# saving parameter using tf.train.CheckpointManager\n",
    "\n",
    "SAVER_DIR = \"./model\"\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(0), model = CNN_model)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, directory=SAVER_DIR ,max_to_keep=5)\n",
    "latest_ckpt = tf.train.latest_checkpoint(SAVER_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c13b04",
   "metadata": {
    "id": "31c13b04"
   },
   "source": [
    "## Restore model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc486a81",
   "metadata": {
    "id": "fc486a81",
    "outputId": "825ac1c1-a593-4a56-f163-f2a96cb23f8b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if there is a saved model and parameter then restore it\n",
    "# and by using it print precision of test dataset and quit the program\n",
    "if latest_ckpt:\n",
    "    ckpt.restore(latest_ckpt)\n",
    "    print(\"Accuracy (Restored): %f\" % compute_accuracy(CNN_model(x_test)[0], y_test))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cb2ac7",
   "metadata": {
    "id": "22cb2ac7",
    "outputId": "fa23bf7b-6922-4b2e-a7cf-dcae3cb2caec"
   },
   "outputs": [],
   "source": [
    "# optimize for 10000 step\n",
    "while int(ckpt.step) < (10000 + 1):\n",
    "    # load the 50 MNIST data at a time\n",
    "    batch_x, batch_y = next(train_data_iter)\n",
    "    \n",
    "    # print accuracy of train dataset in each 100 step, then save parameter using tf.train.CheckpointManager\n",
    "    if ckpt.step % 100 == 0:\n",
    "        ckpt_manager.save(checkpoint_number=ckpt.step)\n",
    "        train_accuracy = compute_accuracy(CNN_model(batch_x)[0], batch_y)\n",
    "        print(\"Epoch: %d, Accuracy of training data: %f\" % (ckpt.step, train_accuracy))\n",
    "        \n",
    "    # update parameter using optimizer\n",
    "    train_step(CNN_model, batch_x, batch_y)\n",
    "    ckpt.step.assign_add(1)\n",
    "\n",
    "# if training is end, then print the accuracy\n",
    "print(\"Accuracy (Restored): %f\" % compute_accuracy(CNN_model(x_test)[0], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cbf672",
   "metadata": {
    "id": "b2cbf672"
   },
   "source": [
    "# 2. Exercise of using TenserBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767840bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "767840bd",
    "outputId": "e39bd148-9eb4-4c4d-b327-8a7d68f7e290"
   },
   "outputs": [],
   "source": [
    "# TensorBoard 예제\n",
    "# Convolutional Neural Networks(CNN)을 이용한 MNIST 분류기(Classifier) - Keras API를 이용한 구현\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# MNIST 데이터를 다운로드 합니다.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# 이미지들을 float32 데이터 타입으로 변경합니다.\n",
    "x_train, x_test = x_train.astype('float32'), x_test.astype('float32')\n",
    "# 28*28 형태의 이미지를 784차원으로 flattening 합니다.\n",
    "x_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1, 784])\n",
    "# [0, 255] 사이의 값을 [0, 1]사이의 값으로 Normalize합니다.\n",
    "x_train, x_test = x_train / 255., x_test / 255.\n",
    "# 레이블 데이터에 one-hot encoding을 적용합니다.\n",
    "y_train, y_test = tf.one_hot(y_train, depth=10), tf.one_hot(y_test, depth=10)\n",
    "\n",
    "# tf.data API를 이용해서 데이터를 섞고 batch 형태로 가져옵니다.\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat().shuffle(60000).batch(50)\n",
    "train_data_iter = iter(train_data)\n",
    "\n",
    "# tf.keras.Model을 이용해서 CNN 모델을 정의합니다.\n",
    "class CNN(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(CNN, self).__init__()\n",
    "    # 첫번째 Convolution Layer\n",
    "    # 5x5 Kernel Size를 가진 32개의 Filter를 적용합니다.\n",
    "    self.conv_layer_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=5, strides=1, padding='same', activation='relu')\n",
    "    self.pool_layer_1 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2)\n",
    "\n",
    "    # 두번째 Convolutional Layer\n",
    "    # 5x5 Kernel Size를 가진 64개의 Filter를 적용합니다.\n",
    "    self.conv_layer_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=5, strides=1, padding='same', activation='relu')\n",
    "    self.pool_layer_2 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2)\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    # 7x7 크기를 가진 64개의 activation map을 1024개의 특징들로 변환합니다.\n",
    "    self.flatten_layer = tf.keras.layers.Flatten()\n",
    "    self.fc_layer_1 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "\n",
    "    # Output Layer\n",
    "    # 1024개의 특징들(feature)을 10개의 클래스-one-hot encoding으로 표현된 숫자 0~9-로 변환합니다.\n",
    "    self.output_layer = tf.keras.layers.Dense(10, activation=None)\n",
    "\n",
    "  def call(self, x):\n",
    "    # MNIST 데이터를 3차원 형태로 reshape합니다. MNIST 데이터는 grayscale 이미지기 때문에 3번째차원(컬러채널)의 값은 1입니다.\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    # 28x28x1 -> 28x28x32\n",
    "    h_conv1 = self.conv_layer_1(x_image)\n",
    "    # 28x28x32 -> 14x14x32\n",
    "    h_pool1 = self.pool_layer_1(h_conv1)\n",
    "    # 14x14x32 -> 14x14x64\n",
    "    h_conv2 = self.conv_layer_2(h_pool1)\n",
    "    # 14x14x64 -> 7x7x64\n",
    "    h_pool2 = self.pool_layer_2(h_conv2)\n",
    "    # 7x7x64(3136) -> 1024\n",
    "    h_pool2_flat = self.flatten_layer(h_pool2)\n",
    "    h_fc1 = self.fc_layer_1(h_pool2_flat)\n",
    "    # 1024 -> 10\n",
    "    logits = self.output_layer(h_fc1)\n",
    "    y_pred = tf.nn.softmax(logits)\n",
    "\n",
    "    return y_pred, logits\n",
    "\n",
    "# cross-entropy 손실 함수를 정의합니다.\n",
    "@tf.function\n",
    "def cross_entropy_loss(logits, y):\n",
    "  return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "\n",
    "# 최적화를 위한 Adam 옵티마이저를 정의합니다.\n",
    "optimizer = tf.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8048804d",
   "metadata": {
    "id": "8048804d"
   },
   "source": [
    "## Code for TensofBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f41fe3",
   "metadata": {
    "id": "d5f41fe3"
   },
   "outputs": [],
   "source": [
    "# Set the directory of folder for summary info. and delcare FIlewriter\n",
    "train_summary_writer = tf.summary.create_file_writer('./tensorboard_log/train')\n",
    "test_summary_writer = tf.summary.create_file_writer('./tesnsorboard_log/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17823c0",
   "metadata": {
    "id": "b17823c0"
   },
   "outputs": [],
   "source": [
    "# function for optimization\n",
    "def train_step(model, x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred, logits = model(x)\n",
    "        loss = cross_entropy_loss(logits, y)\n",
    "    # save tf.summary.sclar, tf.summary.image tensor log at each step\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', loss, step=optimizer.iterations)\n",
    "        x_image = tf.reshape(x, [-1,28,28,1])\n",
    "        tf.summary.image('training image', x_image, max_outputs=10, step=optimizer.iterations)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "# function for printing accuracy\n",
    "def compute_accuracy(y_pred, y, summary_writer):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('accuracy', accuracy, step=optimizer.iterations)\n",
    "        \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67746a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Networks(CNN) 모델을 선언합니다.\n",
    "CNN_model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c340b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizing for 10000 steps\n",
    "for i in range(10000):\n",
    "    # call 50 MNIST data\n",
    "    batch_x, batch_y = next(train_data_iter)\n",
    "    \n",
    "    # print accuracy at each 100 steps\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = compute_accuracy(CNN_model(batch_x)[0], batch_y, train_summary_writer)\n",
    "        print(\"Epoch: %d, Training Accuracy: %f\" % (i, train_accuracy))\n",
    "        \n",
    "    # updating parameter using optimizer\n",
    "    train_step(CNN_model, batch_x, batch_y)\n",
    "    \n",
    "#print Accuracy after fininshed trining\n",
    "print(\"Accuracy: %f\" % compute_accuracy(CNN_model(x_test)[0], y_test, test_summary_writer))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
